{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeEnvironment:\n",
    "    def __init__(self):\n",
    "    # Define the maze layout, rewards, action space (up, down, left, right)\n",
    "        self.start_pos = (0,0) # Start position of the agent\n",
    "        self.current_pos = self.start_pos\n",
    "        self.state_penalty = -1\n",
    "        self.trap_penalty = -100\n",
    "        self.goal_reward = 100\n",
    "        self.actions = {0:(-1, 0), 1:(1, 0), 2:(0, -1), 3:(0, 1)}\n",
    "        self.maze = np.array([[0,1,1,1,1,1,1,0,0,1,1],\n",
    "                                    [0,0,0,0,1,1,0,2,0,0,1],\n",
    "                                    [0,1,1,0,0,0,0,1,0,1,1],\n",
    "                                    [0,1,1,0,1,1,1,1,0,0,0],\n",
    "                                    [0,0,1,0,0,0,1,1,0,1,0],\n",
    "                                    [0,1,1,1,1,0,0,1,0,1,3],\n",
    "                                    [0,0,0,0,0,2,1,0,0,0,1],\n",
    "                                    [1,0,1,0,1,0,0,0,1,1,0],\n",
    "                                    [1,0,1,1,1,1,1,0,0,1,0],\n",
    "                                    [1,0,0,0,0,1,1,1,0,1,0],\n",
    "                                    [1,1,1,1,0,0,0,0,0,0,0]])\n",
    "        self.number_rows, self.number_columns = self.maze.shape\n",
    "    def reset(self):\n",
    "        self.current_pos = self.start_pos\n",
    "        \n",
    "    def step(self, action):\n",
    "        prob = random.random()\n",
    "        if prob < 0.75:\n",
    "            new_move = self.actions[action]\n",
    "        elif prob < 0.80:\n",
    "            new_move = (-self.actions[action][0],-self.actions[action][1])\n",
    "        elif prob < 0.90:\n",
    "            new_move = (self.actions[action][1], self.actions[action][0])\n",
    "        else:\n",
    "            new_move = (-self.actions[action][1], -self.actions[action][0])\n",
    "        new_position = (self.current_pos[0] + new_move[0] , self.current_pos[1] + new_move[1])\n",
    "        \n",
    "        done = False\n",
    "        reward = 0\n",
    "        \n",
    "        # check the boundaries & obstacles\n",
    "        if new_position[0] > self.number_rows-1 or new_position[0] < 0 or new_position[1] > self.number_columns-1 or new_position[1] < 0:\n",
    "            self.current_pos = self.current_pos\n",
    "            reward = -1\n",
    "        elif self.maze[new_position] == 1:\n",
    "            self.current_pos = self.current_pos\n",
    "            reward = -1\n",
    "        else:\n",
    "            self.current_pos = new_position\n",
    "            if self.maze[self.current_pos] == 0:\n",
    "                reward = 0\n",
    "                done = False\n",
    "            elif self.maze[self.current_pos] == 1:\n",
    "                print(\"error the position must not be an obstacle\")\n",
    "            elif self.maze[self.current_pos] == 2:\n",
    "                \n",
    "                reward = -100\n",
    "                done = True\n",
    "            elif self.maze[self.current_pos] == 3:\n",
    "                reward = 100\n",
    "                done = True\n",
    "                \n",
    "        return reward, done\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "def plot_value_function(value_function, maze, alpha, gamma, epsilon, num_episode, method):\n",
    "    mask = np.zeros_like(value_function, dtype=bool)\n",
    "    mask[maze == 1] = True  # Mask obstacles\n",
    "    mask[maze == 2] = True  # Mask the trap\n",
    "    mask[maze == 3] = True  # Mask the goal\n",
    "\n",
    "    trap_position = tuple(np.array(np.where(maze == 2)).transpose(1, 0))\n",
    "    goal_position = np.where(maze == 3)\n",
    "    obs_position = tuple(np.array(np.where(maze == 1)).transpose(1, 0))\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    cmap = LinearSegmentedColormap.from_list('rg', [\"r\", \"w\", \"g\"], N=256)\n",
    "    ax = sns.heatmap(value_function, mask=mask, annot=True, fmt=\".1f\", cmap=cmap,\n",
    "                     cbar=False, linewidths=1, linecolor='black')\n",
    "    ax.add_patch(plt.Rectangle(goal_position[::-1], 1, 1, fill=True, edgecolor='black', facecolor='darkgreen'))\n",
    "    for t in trap_position:\n",
    "        ax.add_patch(plt.Rectangle(t[::-1], 1, 1, fill=True, edgecolor='black', facecolor='darkred'))\n",
    "    for o in obs_position:\n",
    "        ax.add_patch(plt.Rectangle(o[::-1], 1, 1, fill=True, edgecolor='black', facecolor='gray'))\n",
    "    ax.set_title(f\"Value Function for alpha={alpha}, gamma={gamma}, epsilon={epsilon}, episode={num_episode}, method={method}\")\n",
    "\n",
    "    folder_name = f\"plots_alpha_{alpha}_gamma_{gamma}_epsilon_{epsilon}_method_{method}\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    plt.savefig(os.path.join(folder_name, f\"episode_{num_episode}, value_function.png\"))\n",
    "    plt.close()\n",
    "\n",
    "def plot_policy(value_function, maze, alpha, gamma, epsilon, num_episode, method):\n",
    "    policy_arrows = {'up': '↑', 'down': '↓', 'left': '←', 'right': '→'}\n",
    "    policy_grid = np.full(maze.shape, '', dtype='<U2')\n",
    "    actions = ['up', 'down', 'left', 'right']\n",
    "\n",
    "    trap_position = tuple(np.array(np.where(maze == 2)).transpose(1, 0))\n",
    "    goal_position = np.where(maze == 3)\n",
    "    obs_position = tuple(np.array(np.where(maze == 1)).transpose(1, 0))\n",
    "\n",
    "    for i in range(maze.shape[0]):\n",
    "        for j in range(maze.shape[1]):\n",
    "            if maze[i][j] == 1 or (i, j) == goal_position:\n",
    "                continue  # Skip obstacles and the goal\n",
    "            best_action = None\n",
    "            best_value = float('-inf')\n",
    "            for action in actions:\n",
    "                next_i, next_j = i, j\n",
    "                if action == 'up':\n",
    "                    next_i -= 1\n",
    "                elif action == 'down':\n",
    "                    next_i += 1\n",
    "                elif action == 'left':\n",
    "                    next_j -= 1\n",
    "                elif action == 'right':\n",
    "                    next_j += 1\n",
    "                if 0 <= next_i < maze.shape[0] and 0 <= next_j < maze.shape[1]:\n",
    "                    if value_function[next_i][next_j] > best_value:\n",
    "                        best_value = value_function[next_i][next_j]\n",
    "                        best_action = action\n",
    "            if best_action:\n",
    "                policy_grid[i][j] = policy_arrows[best_action]\n",
    "\n",
    "    mask = np.zeros_like(value_function, dtype=bool)\n",
    "    mask[maze == 1] = True  # Mask obstacles\n",
    "    mask[maze == 2] = True  # Mask the trap\n",
    "    mask[maze == 3] = True  # Mask the goal\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    cmap = LinearSegmentedColormap.from_list('rg', [\"r\", \"w\", \"g\"], N=256)\n",
    "    ax = sns.heatmap(value_function, mask=mask, annot=policy_grid, fmt=\"\", cmap=cmap,\n",
    "                     cbar=False, linewidths=1, linecolor='black')\n",
    "    ax.add_patch(plt.Rectangle(goal_position[::-1], 1, 1, fill=True, edgecolor='black', facecolor='darkgreen'))\n",
    "    for t in trap_position:\n",
    "        ax.add_patch(plt.Rectangle(t[::-1], 1, 1, fill=True, edgecolor='black', facecolor='darkred'))\n",
    "    for o in obs_position:\n",
    "        ax.add_patch(plt.Rectangle(o[::-1], 1, 1, fill=True, edgecolor='black', facecolor='gray'))\n",
    "    ax.set_title(f\"Policy Map for alpha={alpha}, gamma={gamma}, epsilon={epsilon}, episode={num_episode}, method={method}\")\n",
    "\n",
    "    folder_name = f\"plots_alpha_{alpha}_gamma_{gamma}_epsilon_{epsilon}_method_{method}\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    plt.savefig(os.path.join(folder_name, f\"episode_{num_episode}, policy.png\"))\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeTD0(MazeEnvironment):  # Inherits from MazeEnvironment\n",
    "    def __init__(self, alpha=0.1, gamma=0.95, epsilon=0.2, episodes=10000):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Learning Rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration Rate\n",
    "        self.episodes = episodes\n",
    "        self.utility = np.zeros(self.maze.shape)  # Initialize utility values to zero\n",
    "\n",
    "    def choose_action(self):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(list(self.actions.keys()))\n",
    "        else:\n",
    "            utilities = []\n",
    "            for action in self.actions.keys():\n",
    "                \n",
    "                reward, done = self.step(action)\n",
    "                utilities.append(self.utility[self.current_pos] if not done else reward)\n",
    "            return np.argmax(utilities)\n",
    "\n",
    "    def update_utility_value(self):\n",
    "        done = False\n",
    "        while not done:\n",
    "            current_state = self.current_pos\n",
    "            current_value = self.utility[current_state]\n",
    "            action = self.choose_action()\n",
    "            reward, done = self.step(action)\n",
    "            new_state = self.current_pos\n",
    "            new_value = reward + self.gamma * self.utility[new_state]\n",
    "            self.utility[current_state] += self.alpha * (new_value - current_value)\n",
    "\n",
    "    def run_episodes(self):\n",
    "        for j in range(1, self.episodes+1):\n",
    "            self.reset()\n",
    "            self.update_utility_value()\n",
    "            if self.episodes == 10000:\n",
    "                if j == 1 or j == 50 or j == 100 or j ==1000 or j ==  5000 or j == 10000:\n",
    "                    plot_value_function(value_function=self.utility, maze=self.maze, alpha = self.alpha, gamma = self.gamma, epsilon=self.epsilon, num_episode=j, method = \"Temporal Difference Learning\")\n",
    "                    plot_policy(value_function= self.utility, maze=self.maze, alpha = self.alpha, gamma = self.gamma, epsilon=self.epsilon, num_episode=j, method= \"Temporal Difference Learning\")\n",
    "        return self.utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_agent = MazeTD0(alpha=0.1, gamma=0.95, epsilon=0.2, episodes=100000)\n",
    "final_utility = td_agent.run_episodes()\n",
    "plot_value_function(value_function=final_utility, maze=td_agent.maze, alpha = 0.1, gamma = 0.95, epsilon=0.2, num_episode=100000)\n",
    "plot_policy(value_function=final_utility, maze=td_agent.maze, alpha = 0.1, gamma = 0.95, epsilon=0.2, num_episode=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MazeQLearning(MazeEnvironment):  # Inherits from MazeEnvironment\n",
    "    def __init__(self, alpha=0.1, gamma=0.95, epsilon=0.2, episodes=10000):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha  # Learning Rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.epsilon = epsilon  # Exploration Rate\n",
    "        self.episodes = episodes\n",
    "        self.q_table = np.zeros((self.number_rows, self.number_columns, len(self.actions)))  # Initialize Q-table\n",
    "\n",
    "    def choose_action(self):\n",
    "        state = self.current_pos\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(list(self.actions.keys()))\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state[0], state[1], :])\n",
    "\n",
    "    def update_q_table(self):\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            current_state = self.current_pos\n",
    "            action = self.choose_action()\n",
    "            reward, done = self.step(action)\n",
    "            current_q = self.q_table[current_state[0], current_state[1], action]\n",
    "            new_state = self.current_pos\n",
    "            max_future_q = np.max(self.q_table[new_state[0], new_state[1], :])\n",
    "            new_q = reward + self.gamma * max_future_q\n",
    "            self.q_table[current_state[0], current_state[1], action] += self.alpha * (new_q - current_q)\n",
    "\n",
    "    def run_episodes(self):\n",
    "        for j in range(1,self.episodes+1):\n",
    "            self.reset()\n",
    "            self.update_q_table()\n",
    "            if j == 1 or j == 50 or j == 100 or j ==1000 or j ==  5000 or j == 10000:\n",
    "                plot_value_function(value_function=np.max(self.q_table, axis=2), maze=self.maze, alpha = self.alpha, gamma = self.gamma, epsilon=self.epsilon, num_episode=j, method='Q_learning')\n",
    "                plot_policy(value_function= np.max(self.q_table, axis=2), maze=self.maze, alpha = self.alpha, gamma = self.gamma, epsilon=self.epsilon, num_episode=j, method= \"Q_learning\")\n",
    "                \n",
    "        return np.max(self.q_table, axis=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "q_learning_agent = MazeQLearning(alpha=0.1, gamma=0.95, epsilon=0.2, episodes=10000)\n",
    "final_q_table = q_learning_agent.run_episodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [0.001, 0.01, 0.1, 0.5, 1]\n",
    "for alpha in alpha_values:\n",
    "    td_agent = MazeTD0(alpha=alpha, gamma=0.95, epsilon=0.2, episodes=10000)\n",
    "    final_utility = td_agent.run_episodes()\n",
    "    maze = td_agent.maze\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_values = [0.1, 0.25, 0.5, 0.75]\n",
    "for gamma in gamma_values:\n",
    "    td_agent = MazeTD0(alpha=0.1, gamma=gamma, epsilon=0.2, episodes=10000)\n",
    "    final_utility = td_agent.run_episodes()\n",
    "    maze = td_agent.maze\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_values = [0, 0.5, 0.8, 1]\n",
    "for epsilon in epsilon_values:\n",
    "    td_agent = MazeTD0(alpha=0.1, gamma=0.95, epsilon=epsilon, episodes=10000)\n",
    "    final_utility = td_agent.run_episodes()\n",
    "    maze = td_agent.maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [0.001, 0.01, 0.1, 0.5, 1]\n",
    "for alpha in alpha_values:\n",
    "    q_learning_agent = MazeQLearning(alpha=alpha, gamma=0.95, epsilon=0.2, episodes=10000)\n",
    "    final_q_table = q_learning_agent.run_episodes()\n",
    "    maze = q_learning_agent.maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma_values = [0.1, 0.25, 0.5, 0.75]\n",
    "for gamma in gamma_values:\n",
    "    q_learning_agent = MazeQLearning(alpha=0.1, gamma=gamma, epsilon=0.2, episodes=10000)\n",
    "    final_q_table = q_learning_agent.run_episodes()\n",
    "    maze = q_learning_agent.maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_values = [0, 0.5, 0.8, 1]\n",
    "for epsilon in epsilon_values:\n",
    "    q_learning_agent = MazeQLearning(alpha=0.1, gamma=0.95, epsilon=epsilon, episodes=10000)\n",
    "    final_q_table = q_learning_agent.run_episodes()\n",
    "    maze = q_learning_agent.maze\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
